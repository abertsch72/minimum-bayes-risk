{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-xsum')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-xsum')\n",
    "device = 'cuda:0'\n",
    "device = 'cpu'\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 1.51k/1.51k [00:00<00:00, 1.20MB/s]\n",
      "Downloading:  65%|██████▍   | 1.05G/1.63G [00:51<00:22, 25.4MB/s]"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# util\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import statistics\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "import multiprocessing\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BartForConditionalGeneration, BartModel, BartTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "logger = logging.getLogger('sum')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# create file handler which logs even debug messages\n",
    "fh = logging.FileHandler(f\"{now.strftime('%m')}{now.strftime('%d')}.html\")\n",
    "fh.setLevel(logging.DEBUG)\n",
    "# create console handler with a higher log level\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "# create formatter and add it to the handlers\n",
    "formatter = logging.Formatter('<br>%(levelname)s - %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "fh.setFormatter(formatter)\n",
    "# add the handlers to logger\n",
    "logger.addHandler(ch)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "from util import *\n",
    "\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "def write_pkl_to_disk(path: str, fname_prefix: str, data_obj):\n",
    "    full_fname = os.path.join(path, f\"{fname_prefix}.pkl\")\n",
    "    with open(full_fname, 'wb') as fd:\n",
    "        pickle.dump(data_obj, fd)\n",
    "    logging.debug(f\"Done writing to {full_fname}\")\n",
    "\n",
    "\n",
    "def init_bart_sum_model(mname='sshleifer/distilbart-cnn-6-6', device='cuda:0'):\n",
    "    model = BartForConditionalGeneration.from_pretrained(mname).to(device)\n",
    "    tokenizer = BartTokenizer.from_pretrained(mname)\n",
    "    return model, tokenizer\n",
    "\n",
    "def bart_decoder_forward_embed(input_ids, embed_tokens, embed_scale):\n",
    "    input_shape = input_ids.size()\n",
    "    input_ids = input_ids.view(-1, input_shape[-1])\n",
    "    inputs_embeds = embed_tokens(input_ids) * embed_scale\n",
    "    return inputs_embeds\n",
    "\n",
    "\n",
    "def summarize_attributions(attributions):\n",
    "    attributions = attributions.mean(dim=-1)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    return attributions\n",
    "\n",
    "def forward_enc_dec_step(model, encoder_outputs, decoder_inputs_embeds):\n",
    "    # expanded_batch_idxs = (\n",
    "    #         torch.arange(batch_size)\n",
    "    #             .view(-1, 1)\n",
    "    #             .repeat(1, 1)\n",
    "    #             .view(-1)\n",
    "    #             .to(device)\n",
    "    #     )\n",
    "    # encoder_outputs[\"last_hidden_state\"] = encoder_outputs.last_hidden_state.index_select(\n",
    "    #         0, expanded_batch_idxs\n",
    "    #     )\n",
    "    model_inputs = {\"input_ids\": None,\n",
    "                    \"past_key_values\": None,\n",
    "                    \"encoder_outputs\": encoder_outputs,\n",
    "                    \"decoder_inputs_embeds\": decoder_inputs_embeds,\n",
    "                    }\n",
    "    outputs = model(**model_inputs, use_cache=False,\n",
    "                    return_dict=True, output_attentions=True)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def init_bart_family(name_lm, name_sum, device, no_lm=False, no_ood=False):\n",
    "    if not no_lm:\n",
    "        lm_model, tok = init_bart_lm_model(name_lm, device)\n",
    "    else:\n",
    "        lm_model = None\n",
    "    sum_model, tok = init_bart_sum_model(name_sum, device)\n",
    "    if not no_ood:\n",
    "        if name_sum == \"facebook/bart-large-cnn\": \n",
    "            sum_out_of_domain, _ = init_bart_sum_model(\n",
    "            \"facebook/bart-large-xsum\", device)\n",
    "        else:\n",
    "            sum_out_of_domain, _ = init_bart_sum_model(\n",
    "            \"facebook/bart-large-cnn\", device) \n",
    "    else:\n",
    "        sum_out_of_domain = None\n",
    "    return lm_model, sum_model, sum_out_of_domain, tok\n",
    "\n",
    "from captum.attr._utils.visualization import format_word_importances\n",
    "\n",
    "\n",
    "def simple_viz_attribution(tokenizer, input_ids, attribution_scores):\n",
    "    token_in_list = input_ids.tolist()\n",
    "    if isinstance(token_in_list[0], list):\n",
    "        token_in_list = token_in_list[0]\n",
    "    words = [tokenizer.decode(x) for x in token_in_list]\n",
    "    attribution_scores_list = attribution_scores.tolist()\n",
    "    # for w, ascore in zip(words, attribution_scores_list):\n",
    "    #     logging.info('{:10} {:02.2f}'.format(w, ascore))\n",
    "\n",
    "    output = format_word_importances(words, attribution_scores_list)\n",
    "    return output\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_full_model_slim(model, input_ids, attention_mask=None, decoder_input_ids=None, targets=None, device='cuda:0', output_dec_hid=False, output_attentions=False, T=1, special_attn=False):\n",
    "    decoder_input_ids = decoder_input_ids.to(device)\n",
    "    input_ids = input_ids.to(device)\n",
    "    if attention_mask is not None:\n",
    "        attention_mask = attention_mask.to(device)\n",
    "    assert decoder_input_ids.size()[0] == input_ids.size()[0]\n",
    "\n",
    "    model_inputs = {\"input_ids\": input_ids,\n",
    "                    \"attention_mask\": attention_mask,\n",
    "                    \"decoder_input_ids\": decoder_input_ids,\n",
    "                    }\n",
    "\n",
    "    outputs = model(**model_inputs,\n",
    "                    output_hidden_states=output_dec_hid, output_attentions=output_attentions,\n",
    "                    use_cache=False, return_dict=True)\n",
    "\n",
    "    # batch, dec seq, vocab size\n",
    "    next_token_logits = outputs.logits[:, -1, :]\n",
    "    if targets is not None:\n",
    "        targets = targets.to(device)\n",
    "        loss = torch.nn.functional.cross_entropy(\n",
    "            input=next_token_logits, target=targets, reduction='none')\n",
    "    else:\n",
    "        loss = 0\n",
    "    if special_attn:\n",
    "        cross_attn = outputs['cross_attentions']\n",
    "        attn = cross_attn[-1][:, :, -1, :]\n",
    "        # batch, nhead, enc_len\n",
    "        mean_attn = torch.mean(attn, dim=1)\n",
    "        # block special positions in input\n",
    "        mask = (input_ids >= 5).float()\n",
    "        mean_attn = mean_attn * mask\n",
    "        return mean_attn[0] \n",
    "    if output_attentions:\n",
    "        # use cross attention as the distribution\n",
    "        # last layer.   batch=1, head, dec len, enc len\n",
    "        # by default we use the last layer of attention\n",
    "        output, p = get_cross_attention(\n",
    "            outputs['cross_attentions'], input_ids, device=device)\n",
    "        return output, p\n",
    "\n",
    "    \n",
    "    prob = torch.nn.functional.softmax(next_token_logits/T, dim=-1)\n",
    "    # prob = next_token_logits.softmax(dim=-1)\n",
    "    next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "    # next_token = next_token.unsqueeze(-1)\n",
    "    next_token = next_token.tolist()    # confrim nested list?\n",
    "    # print(f\"Gold: {tokenizer.decode(targets[0].item())}\")\n",
    "    output = [tokenizer.decode(tk) for tk in next_token]\n",
    "    # logging.info(f\"Next token: {output}\")\n",
    "    # outputs['output'] = output\n",
    "    return output, prob, next_token_logits, loss\n",
    "from scipy.stats import entropy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ARTICLE_TO_SUMMARIZE = \"Police in suburban Minneapolis shot and killed a man when what where who was allegedly involved in a carjacking and fired shots at pursuing officers, according to a release from the Burnsville Police Department. When, when.\"\n",
    "inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')\n",
    "# Generate Summary\n",
    "summary_ids = model.generate(inputs['input_ids'], num_beams=1, max_length=100, early_stopping=True)\n",
    "print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])\n",
    "\n",
    "summary_ids = model.generate(inputs['input_ids'], num_beams=10, max_length=100, early_stopping=True)\n",
    "print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])\n",
    "\n",
    "summary_ids = model.generate(inputs['input_ids'], num_beams=100, max_length=100, early_stopping=True)\n",
    "print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def yield_fact_examples_from_curated_artifact(path_hall = '/mnt/data1/jcxu/back_to_fact/artifact',file_hall='artifact_hallucination.json'):\n",
    "    import json\n",
    "    with open(os.path.join(path_hall,file_hall), 'r') as fd:\n",
    "        hall_data = json.load(fd)\n",
    "    yield hall_data\n",
    "yielder = yield_fact_examples_from_curated_artifact()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "device='cpu'\n",
    "\n",
    "for hall_case in next(yielder):\n",
    "\n",
    "    document = hall_case['input']\n",
    "    doc_input_ids = tokenizer(document, return_tensors='pt')['input_ids'][:,:600]\n",
    "    prefix = hall_case['prefix']\n",
    "    prefix_token_ids = tokenizer(prefix)['input_ids'][:-1]\n",
    "    \n",
    "    logger.info(\"*\"*100)\n",
    "    logger.info(f\"<strong>Prefix: {prefix[1:]}</strong>\")\n",
    "    last_pred = None\n",
    "    for t in range(10):\n",
    "        logger.info('-'*10)\n",
    "        if last_pred:\n",
    "            prefix_token_ids += last_pred\n",
    "            \n",
    "        # target_token = tokens[idx]\n",
    "        # tgt_token_id = tokenizer(\" \" +target_token)['input_ids'][1]\n",
    "\n",
    "        logger.info(f\"<<strong>>Prefix: {tokenizer.decode(prefix_token_ids,skip_special_tokens=True)}\\t </strong>\")\n",
    "        # prefix_ids = tokenizer(' '.join(prefix_tokens),return_tensors='pt')\n",
    "        decoder_input_ids = torch.tensor([prefix_token_ids], dtype=torch.long)\n",
    "        # print(decoder_input_ids)\n",
    "        # print(doc_input_ids)\n",
    "        output, prob, next_token_logits, loss = run_full_model_slim(model=model, input_ids=doc_input_ids, decoder_input_ids=decoder_input_ids, device=device)\n",
    "        # entropy\n",
    "        squeeze_prob = prob.squeeze()\n",
    "        show_top_k(squeeze_prob, tokenizer= tokenizer)\n",
    "        ent_of_pred = entropy(squeeze_prob.cpu().numpy())\n",
    "        logger.info(f\"Entropy: {ent_of_pred: 8.2f}\")\n",
    "        last_pred = [torch.argmax(squeeze_prob).tolist()]"
   ],
   "outputs": [],
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "document = \"The RoBERTa model was proposed in RoBERTa: A Robustly Optimized BERT Pretraining Approach by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov. It is based on Google’s BERT model released in 2018. It builds on BERT and modifies key hyperparameters, removing the next-sentence pretraining objective and training with much larger mini-batches and learning rates.\"\n",
    "device = 'cpu'\n",
    "doc_input_ids = tokenizer(document, return_tensors='pt')['input_ids'][:,:600]\n",
    "decoder_input_ids = torch.tensor([[0]], dtype=torch.long)\n",
    "# print(doc_input_ids.size())\n",
    "# print(decoder_input_ids.size())\n",
    "\n",
    "output, prob, next_token_logits, loss = run_full_model_slim(model=model, input_ids=doc_input_ids, decoder_input_ids=decoder_input_ids, device=device)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load spacy\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_pos(inp_str):\n",
    "    inp_str = inp_str.strip()\n",
    "    doc = nlp(inp_str)\n",
    "    for token in doc:\n",
    "        return token.pos_\n",
    "        # print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,token.shape_, token.is_alpha, token.is_stop)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# search\n",
    "from scipy.stats import entropy\n",
    "from typing import List\n",
    "\n",
    "class SingleStep():\n",
    "    def __init__(self, parent, t, prob, entropy, token=\"\", token_id=0):\n",
    "        self.parent = parent\n",
    "        self.t = t\n",
    "        self.prob=prob\n",
    "        self.entropy=entropy\n",
    "        self.token = token\n",
    "        self.token_id = token_id\n",
    "        self.is_compressive = False     # a new and blue job vs a job\n",
    "        self.is_finished = False\n",
    "    def classify_fact(self):\n",
    "        pass\n",
    "\n",
    "    def understand_word_type(self):\n",
    "        pass\n",
    "\n",
    "    def get_future_prediction(self):\n",
    "        pass\n",
    "\n",
    "    def get_token_ids(self):\n",
    "        if self.parent == None:\n",
    "            return [self.token_id]\n",
    "        else:\n",
    "            return self.parent.get_token_ids() + [self.token_id]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Generate Path: {tokenizer.decode(self.get_token_ids(),skip_special_tokens=True)} \"\n",
    "\n",
    "    def run(self):\n",
    "        # expand the node, return a list of new nodes (sorted by the score)\n",
    "        prefix_token_ids = self.get_token_ids()\n",
    "        decoder_input_ids = torch.tensor([prefix_token_ids], dtype=torch.long)\n",
    "        next_step_gen = step_generation(model, doc_input_ids,decoder_input_ids,device)\n",
    "        is_collide = entity_collide(next_step_gen, prefix_token_ids)\n",
    "        if is_collide:\n",
    "            return []\n",
    "\n",
    "        # predict even further TODO\n",
    "\n",
    "        # populate\n",
    "        rt_list = []\n",
    "        logger.info(f\"Populating...\")\n",
    "        logger.info(f\"Prefix: {tokenizer.decode(prefix_token_ids,skip_special_tokens=True)}\")\n",
    "        for element in next_step_gen:\n",
    "            new_ele = SingleStep(parent=self,t=self.t+1, prob=element['prob'], entropy=element['ent'], token=element['token'], token_id=element['token_id'])\n",
    "            if element['token'].strip() == '.' or element['token'] == '</s>':\n",
    "                new_ele.is_finished = True\n",
    "            rt_list.append(new_ele)\n",
    "            logger.info(f\"\\t--- Expansion: {element['token']}\")\n",
    "        logger.info(f\"------\")\n",
    "        return rt_list[::-1]\n",
    "\n",
    "ban_list = ['PROPN','NUM']\n",
    "def entity_collide(generated_group:List, prefix_token_ids):\n",
    "    # a bunch of collided entities / hallucination\n",
    "    # could be a more complicated fact classifier\n",
    "    cand_entropy = generated_group[0]['ent']\n",
    "    if cand_entropy < 3:\n",
    "        return False\n",
    "    \n",
    "    poss = [x['pos'] for x in generated_group]\n",
    "    num_ban_pos = sum([ 1 for x in poss if x in ban_list])\n",
    "    ban_rate = num_ban_pos / len(generated_group)\n",
    "    if ban_rate > 0.4:\n",
    "        logger.info(f\"Entity collides! Entropy: {cand_entropy} Prefix:{tokenizer.decode(prefix_token_ids,skip_special_tokens=True)}\")\n",
    "        logger.info(f\"{[x['token'] for x in generated_group]}\")\n",
    "        return True\n",
    "    logger.info(f\"HIgh entropy (wo Collision) Entropy: {cand_entropy} Prefix: {tokenizer.decode(prefix_token_ids,skip_special_tokens=True)}\")\n",
    "    logger.info(f\"{[x['token'] for x in generated_group]}\")\n",
    "    return False\n",
    "\n",
    "def step_generation(model, doc_input_ids, decoder_input_ids, device, acc_prob_threshold = 0.8, max_beam_size = 5):\n",
    "    # print(doc_input_ids.size())\n",
    "    # print(decoder_input_ids.size())\n",
    "    output, prob, next_token_logits, loss = run_full_model_slim(model=model, input_ids=doc_input_ids, decoder_input_ids=decoder_input_ids, device=device)\n",
    "    ent = entropy(prob.squeeze().cpu().numpy())\n",
    "    acc_prob = 0\n",
    "    top_prob, top_index = torch.topk(input=prob, k=max_beam_size)\n",
    "    top_prob = top_prob.squeeze().tolist()\n",
    "    top_index = top_index.squeeze().tolist()\n",
    "    return_pairs = []\n",
    "    for idx in range(max_beam_size):\n",
    "        value = top_prob[idx]\n",
    "        token_id = top_index[idx]\n",
    "        token = tokenizer.decode(token_id)\n",
    "        return_pairs.append(\n",
    "            {'prob':value,\n",
    "            'token':token,\n",
    "            'token_id':token_id,\n",
    "            'ent':ent,\n",
    "            'pos':get_pos(token)\n",
    "            })\n",
    "        # if the accumulated prob > threshold, stop iteration\n",
    "        acc_prob += value\n",
    "        if acc_prob > acc_prob_threshold:\n",
    "            break\n",
    "    # return something with token tokenid and prob\n",
    "    return return_pairs\n",
    "\n",
    "def future_generation(prefix, future_step=3):\n",
    "\n",
    "    pass\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# init\n",
    "\n",
    "document = \"The RoBERTa model was proposed in RoBERTa: A Robustly Optimized BERT Pretraining Approach by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov. It is based on Google’s BERT model released in 2018. It builds on BERT and modifies key hyperparameters, removing the next-sentence pretraining objective and training with much larger mini-batches and learning rates.\"\n",
    "device = 'cpu'\n",
    "doc_input_ids = tokenizer(document, return_tensors='pt')['input_ids'][:,:600]\n",
    "\n",
    "stack = []\n",
    "# print(tokenizer.bos_token)\n",
    "# print(tokenizer.bos_token_id)\n",
    "init_state = SingleStep(None, 0, prob=1.0, entropy=1e-3, token=tokenizer.bos_token, token_id=tokenizer.bos_token_id)\n",
    "stack.append(init_state)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "finished_output = []\n",
    "T = 200\n",
    "t = 0\n",
    "max_steps = 20\n",
    "while stack:\n",
    "    work_node = stack.pop()\n",
    "    return_output = work_node.run()\n",
    "    for rt in return_output:\n",
    "        if rt.is_finished or rt.t >= max_steps:\n",
    "            finished_output.append(rt)\n",
    "        else:\n",
    "            # stack.insert(0,rt)\n",
    "            stack.append(rt)\n",
    "    t+=1\n",
    "    if t >T:\n",
    "        break\n",
    "\n",
    "logger.info(\"Finished output\")\n",
    "[logger.info(x) for x in finished_output]"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from datasets import load_dataset\n",
    "dataset_xsum = load_dataset('xsum',split='validation')\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cnt = 50\n",
    "device = 'cuda:0'\n",
    "model = model.to(device=device)\n",
    "for data in dataset_xsum:\n",
    "    document = data['document']\n",
    "    summary = data['summary']\n",
    "    logger.info('*'*50)\n",
    "    logger.info(f\"Summary: {summary}\")\n",
    "    # init\n",
    "    doc_input_ids = tokenizer(document, return_tensors='pt')['input_ids'][:,:600]\n",
    "\n",
    "    stack = []\n",
    "    init_state = SingleStep(None, 0, prob=1.0, entropy=1e-3, token=tokenizer.bos_token, token_id=tokenizer.bos_token_id)\n",
    "    stack.append(init_state)\n",
    "\n",
    "    finished_output = []\n",
    "    T = 200\n",
    "    t = 0\n",
    "    max_steps = 20\n",
    "    while stack:\n",
    "        work_node = stack.pop()\n",
    "        return_output = work_node.run()\n",
    "        for rt in return_output:\n",
    "            if rt.is_finished or rt.t >= max_steps:\n",
    "                finished_output.append(rt)\n",
    "            else:\n",
    "                # stack.insert(0,rt)\n",
    "                stack.append(rt)\n",
    "        t+=1\n",
    "        if t > T:\n",
    "            break\n",
    "\n",
    "    logger.info(\"Finished output\")\n",
    "    [logger.info(x) for x in finished_output]\n",
    "    cnt -= 1\n",
    "    if cnt <=0:\n",
    "        break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
    "import logging\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-xsum\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-xsum\")\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f518a11ac55f4d3cb26f153e70c63bbb"
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=26.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "23bc6774a2d64332aa6a558af820543b"
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898822.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c0db1ce529b84f9e9faccb040549d96a"
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3831b6c480174926862d0a71bcd365f3"
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1355863.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "print(tokenizer.eos_token)\n",
    "print(tokenizer.pad_token)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "</s>\n",
      "<pad>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "\n",
    "ARTICLE_TO_SUMMARIZE = \"It is one of the most prestigious honors bestowed upon journalists and people in the arts. And today, the Pulitzer prize for journalism went to The Post and Courier newspaper of Charleston, South Carolina  - which has a tiny staff of just 80 and a daily circulation of 85,000. The paper's powerful photo series entitled 'Till Death Do Us Part,' on domestic violence scooped the top award for its exploration into why South Carolina is among the deadliest states for women. Winner: This iconic photo by New York Times photographer Daniel Berehulak, was part of a winning series, and shows James Dorbor, 8, suspected of being infected with Ebola, being carried by medical staff to an Ebola treatment center in Monrovia, Liberia. Death: This photograph released by the Pulitzer Board on 20 April 2015 shows one of the winning photographs by Daniel Berehulak who won the 2015 Pulitzer Prize for Feature Photography. </s> One small newspaper\"\n",
    "\n",
    "# ARTICLE_TO_SUMMARIZE =\"\"\n",
    "\n",
    "inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')\n",
    "\n",
    "# Generate Summary\n",
    "summary_ids = model.generate(inputs['input_ids'], num_beams=10, min_length=60, max_length=60, early_stopping=True)\n",
    "print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[\"One of the world's smallest newspapers has won one of the most prestigious awards in the world of journalism - the Pulitzer Prize for Feature Photography, it has been announced on the eve of the awards ceremony in Washington, D.C. and the winners will be announced in New York on Monday.\"]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "\n",
    "ARTICLE_TO_SUMMARIZE = \"It is one of the most prestigious honors bestowed upon journalists and people in the arts. And today, the Pulitzer prize for journalism went to The Post and Courier newspaper of Charleston, South Carolina  - which has a tiny staff of just 80 and a daily circulation of 85,000. The paper's powerful photo series entitled 'Till Death Do Us Part,' on domestic violence scooped the top award for its exploration into why South Carolina is among the deadliest states for women. Winner: This iconic photo by New York Times photographer Daniel Berehulak, was part of a winning series, and shows James Dorbor, 8, suspected of being infected with Ebola, being carried by medical staff to an Ebola treatment center in Monrovia, Liberia. Death: This photograph released by the Pulitzer Board on 20 April 2015 shows one of the winning photographs by Daniel Berehulak who won the 2015 Pulitzer Prize for Feature Photography\"\n",
    "\n",
    "# ARTICLE_TO_SUMMARIZE =\"\"\n",
    "\n",
    "inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')\n",
    "\n",
    "# Generate Summary\n",
    "summary_ids = model.generate(inputs['input_ids'], num_beams=10, min_length=5, max_length=5, early_stopping=True,num_return_sequences=100)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f9cb647ed887>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ARTICLE_TO_SUMMARIZE =\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mARTICLE_TO_SUMMARIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Generate Summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b2768bf9bd6425acc869120a7136bda7c2db4d32a4c0d82d14afba9d9f0b9d5"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "3b2768bf9bd6425acc869120a7136bda7c2db4d32a4c0d82d14afba9d9f0b9d5"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}