{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.51k/1.51k [00:00<00:00, 1.20MB/s]\n",
      "Downloading:  65%|██████▍   | 1.05G/1.63G [00:51<00:22, 25.4MB/s]"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-xsum')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-xsum')\n",
    "device = 'cuda:0'\n",
    "device = 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset xsum (/home/jcxu/.cache/huggingface/datasets/xsum/default/1.2.0/4957825a982999fbf80bca0b342793b01b2611e021ef589fb7c6250b3577b499)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"xsum\", split='validation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 5161.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'document': ['The country\\'s consumer watchdog has taken Apple to court for false advertising because the tablet computer does not work on Australia\\'s 4G network.\\nApple\\'s lawyers said they were willing to publish a clarification.\\nHowever the company does not accept that it misled customers.\\nThe Australian Competition and Consumer Commission (ACCC) said on Tuesday: \"Apple\\'s recent promotion of the new \\'iPad with wi-fi + 4G\\' is misleading because it represents to Australian consumers that the product can, with a sim card, connect to a 4G mobile data network in Australia, when this is not the case.\"\\nThe watchdog then lodged a complaint at the Federal Court in Melbourne.\\nAt a preliminary hearing, Apple lawyer Paul Anastassiou said Apple had never claimed the device would work fully on the current 4G network operated by Telstra.\\nApple says the new iPad works on what is globally accepted to be a 4G network.\\nThe matter will go to a full trial on 2 May.\\nThe Apple iPad\\'s third version went on sale earlier this month, with Australia the first country where it was available.\\nShoppers lined up by the hundreds at Apple stores on opening day and the company said it had been its strongest iPad launch to date.\\nThe ACCC said it was seeking an injunction on sales as well as a financial penalty against Apple, corrective advertising and refunds to consumers.\\nOn its website, Apple does state that 4G LTE is only supported on selected networks in the US and Canada.', 'No international side has toured Bangladesh since 20 people were killed in a siege at a cafe in Dhaka in July.\\nThe England and Wales Cricket Board said in August that tour would go ahead following a security review.\\nJos Buttler will captain the one-day side, with the squads for the one-day and Test series to be named on Friday.\\nThe tour features three one-day internationals followed by two Test matches.\\nMorgan has followed his own course, with the assurance ringing in his ears that nothing is going to happen, and I don\\'t see how they can now sack him\\nEngland director of cricket Andrew Strauss said: \"While we understand and respect Eoin and Alex\\'s decision, we are disappointed that they have made themselves unavailable for selection for the Bangladesh tour.\"\\nHe added that no further withdrawals are expected following \"open and honest\" discussions with all the players.\\nEngland players Stuart Broad, Moeen Ali, Chris Jordan and Liam Dawson have all publicly said they will tour Bangladesh.\\nFormer England captain Michael Vaughan has described Morgan\\'s decision as a \"huge mistake\", while Nasser Hussain, another ex-skipper of the national side, said Morgan \"should be with his team\".\\nBut team-mate Ben Stokes tweeted his support for both players.\\nEngland fly out on 29 September, with the one-day series starting on 7 October and the Test series on 20 October.\\nThey follow that with a tour of India, which begins on 9 November and features five Tests, three ODIs and three Twenty20s.\\nMorgan, 29, said this week that he would never go on a tour where security concerns may affect his game.\\n\"In 2010, we played an Indian Premier League game in Bangalore and a bomb went off in the ground,\" he said. \"We left and went straight to the airport.\\n\"Another one was playing domestic cricket in Bangladesh during political elections and things were incredibly violent. Given that no-one has toured there since the terrorist attack adds a bigger decision to it.\"\\nBowlers Andrew Caddick and Robert Croft pulled out of England\\'s 2001 tour of India because of safety concerns following the 9/11 attacks in New York.\\n\"I was adamant I didn\\'t want to go and stuck to what I thought was right,\" Caddick told BBC Radio 5 live\\'s Sportsweek programme before Morgan and Hales made their announcement on Sunday.\\n\"It\\'s slightly different now. The security they implement in world cricket is quite high, whereas back in 2001 it wasn\\'t.\\n\"Eoin Morgan has experienced risks to security in the past. Nobody else in the England team has. It\\'s a personal decision and it should stay that way.\"\\nFormer spinner Graeme Swann, who was part of the England squad that returned to India after the Mumbai bombings in 2008, said he would not travel to Bangladesh if he was still playing.\\n\"We are a target,\" he said. \"If I was in that situation, I wouldn\\'t go.\"\\nEx-England skipper Vaughan told BBC Radio 5 live that Morgan should go on the tour.\\n\"I don\\'t think the captain had any other option but to get on the plane and lead a young side. I do think it is a huge mistake that he has made,\" he said.\\n\"This young side will all be nervous because of everything around the side. There will be guards everywhere, they\\'ll have guns in their faces with security outside their rooms. The young England side won\\'t like what they see.\\n\"He\\'ll continue in the side as captain for India - my question would be: \\'Are you safe going there?\\' There is a similar Foreign Office warning.\\n\"There will be a bit of banter created and he\\'ll have to take some stick when he\\'s back in the side, that\\'s the nature of being in a sports team. It\\'s one that I think he will live to regret.\"\\nIt says there is \"a heightened threat of further terrorist attacks\" in Bangladesh, although the threat level applies to many European cities, including London.\\nIt adds \"foreigners, in particular westerners, may be directly targeted\" and that \"places where westerners are known to gather may be at higher risk of attack\".\\nIt advises travellers to Bangladesh to \"minimise exposure to these areas\" and \"remain vigilant, monitor local and social media and follow any specific advice of the local security authorities\".\\nAustralia postponed their Test tour of Bangladesh in October 2015 after the Australian government warned of an increasing security risk to the team.\\nBBC Sport cricket correspondent Jonathan Agnew:\\n\"Morgan has never expressed any positive views on making this tour, so his decision to pull out is not a surprise.\\n\"In his explanation, he refers to a bomb blast in Bangalore in 2010, an incident that profoundly unsettled him, as did civil unrest in Bangladesh. It won\\'t be lost on Strauss, though, that Morgan has returned to play in both countries since.\\n\"However, having guaranteed that players\\' futures will not be affected by this decision, it\\'s difficult to see Morgan losing the captaincy, as has been suggested in some quarters.\\n\"Hales has effectively given another opening batsman the chance to take his place in Test cricket, although his future in that form was doubtful anyway. After a record-breaking summer, he\\'s sure to return to the one-day team in India.\"\\nSabir Mustafa, Editor, BBC Bengali Service:\\n\"The Gulshan cafe attack raised a lot of questions about the police\\'s ability to disrupt the terror network or bring the perpetrators to justice.  A sense of fear had gripped both the expatriate communities as well as the locals who feared an escalation in the frequency and ferocity of attacks.\\n\"But things have moved in the opposite direction lately. Since August, police have located and raided three hideouts of militants in and around Dhaka, killing 13 suspects.\\n\"A sense of confidence is beginning to be generated among the population, that the police may be finally getting to grips with the situation.\\n\"Despite these police successes, caution remains the mantra among expatriates.\"', \"In a nod to Mr Trudeau's fondness for statement socks, he wore Canada-themed mountie and maple leaf print ones to welcome his first foreign visitor.\\nMr Trudeau opted for a more subtle striped pair on his first official engagement in the Republic of Ireland.\\nThe Canadian premier is in the country for a three-day visit.\\nAway from talk of socks, trade issues are set to dominate the agenda, with the Canada-EU free trade deal expected to be a focal point of talks between the two leaders.\\nMr Trudeau arrived in Dublin with his wife, Sophie GrÃ©goire, and their youngest son, Hadrien, on Monday.\\nHe is in Europe for a key G20 summit in Hamburg, Germany later this week.\\nMr Trudeau is also due to meet Irish President Michael D Higgins at his official residence.\\nThe visit was arranged in May when Mr Varadkar's predecessor Enda Kenny met the Canadian leader in Montreal.\\nOn that occasion, Mr Trudeau wore special Star Wars themed socks, as it coincided with the widely-celebrated Star Wars Day on 4 May.\", 'The Subsea Power Hub (SPH) is being tested by Aberdeen based EC-OG at Shapinsay Sound, Orkney.\\nThe company said the system is \"performing in line with technical predictions\".\\nThe unit will be left to operate autonomously over the summer months while it is checked using wireless data monitoring.\\nEC-OG said the SPH is a ground-breaking hybrid drive system which uses a marine energy convertor to produce energy which is stored in a lithium-based unit.\\nEngineering director Robert Cowman said: \"Although the weather was not on our side, we were still able to successfully complete the installation and commence testing ahead of schedule.\\n\"The system is performing very well and I\\'m pleased at how quickly the results are correlating with the theoretical basis for the test. Having a vertical axis turbine, means that the SPH is operating effectively in these unpredictable, sporadic flow conditions.\\n\"Thanks go to the EC-OG team as well as those who have helped us at EMEC, Leask Marine, Castle View and Scottish Enterprise.\"', 'The last quarter saw insolvencies rise year-on-year by 17.3% to 2,839, according to official figures.\\nPaul Wheelhouse claimed there was \"no question\" austerity had led to \"more people suffering the anxiety and distress of insolvency\".\\nThe Scottish Conservatives called on the Scottish government to do more to help people deal with personal debt.\\nAccountant in Bankruptcy (AiB), which administers the process of bankruptcy and records corporate insolvencies, said personal bankruptcies increased by 11.2% year-on-year to 1,289 in the three months to 30 June.\\nProtected trust deeds rose 22.9% over the same period to 1,550.\\nThe number of debt payment programmes under the Debt Arrangement Scheme (DAS) - which allows debtors to pay their debts in full without facing insolvency - were also up by 16.8%, to 597.\\nA total of Â£9.4m was repaid through DAS in the latest quarter, up from Â£9.3m repaid in the same period last year.\\nResponding to the figures, Mr Wheelhouse said: \"On the personal insolvency front, there is no question that continuing austerity has led to incomes being squeezed and more people suffering the anxiety and distress of insolvency as a result.\\n\"But it is nevertheless important we acknowledge that the longer term trend of people accessing statutory debt relief and debt management solutions is a declining oneâ€Ž and numbers of people falling into insolvency are around half of the levels reported at the turn of the decade.\"\\nThe Scottish Conservatives said the personal insolvency figures showed \"just how much some are struggling\".\\nFinance spokesman Murdo Fraser said: \"It\\'s incredibly worrying to see such a sharp increase in the number of Scots going bankrupt.\\n\"Given the lacklustre growth in the Scottish economy over the last year, we are starting to see the consequences of it as more and more people are struggling in a sea of debt.\\n\"These individuals will be going through an incredibly tough time and it is incumbent upon the Scottish government to do all they can to support them.\\n\"One step the SNP could take immediately would be to boost long-term economic growth and job creation by reversing their tax hikes that have made Scotland the highest taxed part of the UK.\"\\nThe latest AiB figures also showed a sharp drop in the number of Scottish businesses becoming insolvent or entering receivership.\\nIn the last quarter, there were 200 corporate insolvencies - a drop of 24.5%.\\nThe figure included 118 compulsory liquidations and 82 creditor voluntary liquidations.'], 'id': ['17532613', '37331990', '40493408', '39601773', '40730958'], 'summary': ['US technology firm Apple has offered to refund Australian customers who felt misled about the 4G capabilities of the new iPad.', \"England one-day captain Eoin Morgan and opening batsman Alex Hales have opted out of October's tour of Bangladesh because of security concerns.\", 'Irish Taoiseach (PM) Leo Varadkar has engaged in some \"sock diplomacy\" in his first meeting with Canadian Prime Minister Justin Trudeau in Dublin.', 'A new ocean current energy system has been successfully deployed, according to the company behind it.', 'Scotland\\'s business minister has blamed \"continuing austerity\" for a sharp rise in personal insolvencies.']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "print(dataset[:5])\n",
    "dataset = dataset[:30]\n",
    "for idx, x in enumerate(tqdm(dataset)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import statistics\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "import multiprocessing\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BartForConditionalGeneration, BartModel, BartTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "logger = logging.getLogger('sum')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# create file handler which logs even debug messages\n",
    "fh = logging.FileHandler(f\"{now.strftime('%m')}{now.strftime('%d')}.html\")\n",
    "fh.setLevel(logging.DEBUG)\n",
    "# create console handler with a higher log level\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "# create formatter and add it to the handlers\n",
    "formatter = logging.Formatter('<br>%(levelname)s - %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "fh.setFormatter(formatter)\n",
    "# add the handlers to logger\n",
    "logger.addHandler(ch)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "from util import *\n",
    "\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "def write_pkl_to_disk(path: str, fname_prefix: str, data_obj):\n",
    "    full_fname = os.path.join(path, f\"{fname_prefix}.pkl\")\n",
    "    with open(full_fname, 'wb') as fd:\n",
    "        pickle.dump(data_obj, fd)\n",
    "    logging.debug(f\"Done writing to {full_fname}\")\n",
    "\n",
    "\n",
    "def init_bart_sum_model(mname='sshleifer/distilbart-cnn-6-6', device='cuda:0'):\n",
    "    model = BartForConditionalGeneration.from_pretrained(mname).to(device)\n",
    "    tokenizer = BartTokenizer.from_pretrained(mname)\n",
    "    return model, tokenizer\n",
    "\n",
    "def bart_decoder_forward_embed(input_ids, embed_tokens, embed_scale):\n",
    "    input_shape = input_ids.size()\n",
    "    input_ids = input_ids.view(-1, input_shape[-1])\n",
    "    inputs_embeds = embed_tokens(input_ids) * embed_scale\n",
    "    return inputs_embeds\n",
    "\n",
    "\n",
    "def summarize_attributions(attributions):\n",
    "    attributions = attributions.mean(dim=-1)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    return attributions\n",
    "\n",
    "def forward_enc_dec_step(model, encoder_outputs, decoder_inputs_embeds):\n",
    "    # expanded_batch_idxs = (\n",
    "    #         torch.arange(batch_size)\n",
    "    #             .view(-1, 1)\n",
    "    #             .repeat(1, 1)\n",
    "    #             .view(-1)\n",
    "    #             .to(device)\n",
    "    #     )\n",
    "    # encoder_outputs[\"last_hidden_state\"] = encoder_outputs.last_hidden_state.index_select(\n",
    "    #         0, expanded_batch_idxs\n",
    "    #     )\n",
    "    model_inputs = {\"input_ids\": None,\n",
    "                    \"past_key_values\": None,\n",
    "                    \"encoder_outputs\": encoder_outputs,\n",
    "                    \"decoder_inputs_embeds\": decoder_inputs_embeds,\n",
    "                    }\n",
    "    outputs = model(**model_inputs, use_cache=False,\n",
    "                    return_dict=True, output_attentions=True)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def init_bart_family(name_lm, name_sum, device, no_lm=False, no_ood=False):\n",
    "    if not no_lm:\n",
    "        lm_model, tok = init_bart_lm_model(name_lm, device)\n",
    "    else:\n",
    "        lm_model = None\n",
    "    sum_model, tok = init_bart_sum_model(name_sum, device)\n",
    "    if not no_ood:\n",
    "        if name_sum == \"facebook/bart-large-cnn\": \n",
    "            sum_out_of_domain, _ = init_bart_sum_model(\n",
    "            \"facebook/bart-large-xsum\", device)\n",
    "        else:\n",
    "            sum_out_of_domain, _ = init_bart_sum_model(\n",
    "            \"facebook/bart-large-cnn\", device) \n",
    "    else:\n",
    "        sum_out_of_domain = None\n",
    "    return lm_model, sum_model, sum_out_of_domain, tok\n",
    "\n",
    "from captum.attr._utils.visualization import format_word_importances\n",
    "\n",
    "\n",
    "def simple_viz_attribution(tokenizer, input_ids, attribution_scores):\n",
    "    token_in_list = input_ids.tolist()\n",
    "    if isinstance(token_in_list[0], list):\n",
    "        token_in_list = token_in_list[0]\n",
    "    words = [tokenizer.decode(x) for x in token_in_list]\n",
    "    attribution_scores_list = attribution_scores.tolist()\n",
    "    # for w, ascore in zip(words, attribution_scores_list):\n",
    "    #     logging.info('{:10} {:02.2f}'.format(w, ascore))\n",
    "\n",
    "    output = format_word_importances(words, attribution_scores_list)\n",
    "    return output\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_full_model_slim(model, input_ids, attention_mask=None, decoder_input_ids=None, targets=None, device='cuda:0', output_dec_hid=False, output_attentions=False, T=1, special_attn=False):\n",
    "    decoder_input_ids = decoder_input_ids.to(device)\n",
    "    input_ids = input_ids.to(device)\n",
    "    if attention_mask is not None:\n",
    "        attention_mask = attention_mask.to(device)\n",
    "    assert decoder_input_ids.size()[0] == input_ids.size()[0]\n",
    "\n",
    "    model_inputs = {\"input_ids\": input_ids,\n",
    "                    \"attention_mask\": attention_mask,\n",
    "                    \"decoder_input_ids\": decoder_input_ids,\n",
    "                    }\n",
    "\n",
    "    outputs = model(**model_inputs,\n",
    "                    output_hidden_states=output_dec_hid, output_attentions=output_attentions,\n",
    "                    use_cache=False, return_dict=True)\n",
    "\n",
    "    # batch, dec seq, vocab size\n",
    "    next_token_logits = outputs.logits[:, -1, :]\n",
    "    if targets is not None:\n",
    "        targets = targets.to(device)\n",
    "        loss = torch.nn.functional.cross_entropy(\n",
    "            input=next_token_logits, target=targets, reduction='none')\n",
    "    else:\n",
    "        loss = 0\n",
    "    if special_attn:\n",
    "        cross_attn = outputs['cross_attentions']\n",
    "        attn = cross_attn[-1][:, :, -1, :]\n",
    "        # batch, nhead, enc_len\n",
    "        mean_attn = torch.mean(attn, dim=1)\n",
    "        # block special positions in input\n",
    "        mask = (input_ids >= 5).float()\n",
    "        mean_attn = mean_attn * mask\n",
    "        return mean_attn[0] \n",
    "    if output_attentions:\n",
    "        # use cross attention as the distribution\n",
    "        # last layer.   batch=1, head, dec len, enc len\n",
    "        # by default we use the last layer of attention\n",
    "        output, p = get_cross_attention(\n",
    "            outputs['cross_attentions'], input_ids, device=device)\n",
    "        return output, p\n",
    "\n",
    "    \n",
    "    prob = torch.nn.functional.softmax(next_token_logits/T, dim=-1)\n",
    "    # prob = next_token_logits.softmax(dim=-1)\n",
    "    next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "    # next_token = next_token.unsqueeze(-1)\n",
    "    next_token = next_token.tolist()    # confrim nested list?\n",
    "    # print(f\"Gold: {tokenizer.decode(targets[0].item())}\")\n",
    "    output = [tokenizer.decode(tk) for tk in next_token]\n",
    "    # logging.info(f\"Next token: {output}\")\n",
    "    # outputs['output'] = output\n",
    "    return output, prob, next_token_logits, loss\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLE_TO_SUMMARIZE = \"Police in suburban Minneapolis shot and killed a man when what where who was allegedly involved in a carjacking and fired shots at pursuing officers, according to a release from the Burnsville Police Department. When, when.\"\n",
    "inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')\n",
    "# Generate Summary\n",
    "summary_ids = model.generate(inputs['input_ids'], num_beams=1, max_length=100, early_stopping=True)\n",
    "print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])\n",
    "\n",
    "summary_ids = model.generate(inputs['input_ids'], num_beams=10, max_length=100, early_stopping=True)\n",
    "print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])\n",
    "\n",
    "summary_ids = model.generate(inputs['input_ids'], num_beams=100, max_length=100, early_stopping=True)\n",
    "print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def yield_fact_examples_from_curated_artifact(path_hall = '/mnt/data1/jcxu/back_to_fact/artifact',file_hall='artifact_hallucination.json'):\n",
    "    import json\n",
    "    with open(os.path.join(path_hall,file_hall), 'r') as fd:\n",
    "        hall_data = json.load(fd)\n",
    "    yield hall_data\n",
    "yielder = yield_fact_examples_from_curated_artifact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "device='cpu'\n",
    "\n",
    "for hall_case in next(yielder):\n",
    "\n",
    "    document = hall_case['input']\n",
    "    doc_input_ids = tokenizer(document, return_tensors='pt')['input_ids'][:,:600]\n",
    "    prefix = hall_case['prefix']\n",
    "    prefix_token_ids = tokenizer(prefix)['input_ids'][:-1]\n",
    "    \n",
    "    logger.info(\"*\"*100)\n",
    "    logger.info(f\"<strong>Prefix: {prefix[1:]}</strong>\")\n",
    "    last_pred = None\n",
    "    for t in range(10):\n",
    "        logger.info('-'*10)\n",
    "        if last_pred:\n",
    "            prefix_token_ids += last_pred\n",
    "            \n",
    "        # target_token = tokens[idx]\n",
    "        # tgt_token_id = tokenizer(\" \" +target_token)['input_ids'][1]\n",
    "\n",
    "        logger.info(f\"<<strong>>Prefix: {tokenizer.decode(prefix_token_ids,skip_special_tokens=True)}\\t </strong>\")\n",
    "        # prefix_ids = tokenizer(' '.join(prefix_tokens),return_tensors='pt')\n",
    "        decoder_input_ids = torch.tensor([prefix_token_ids], dtype=torch.long)\n",
    "        # print(decoder_input_ids)\n",
    "        # print(doc_input_ids)\n",
    "        output, prob, next_token_logits, loss = run_full_model_slim(model=model, input_ids=doc_input_ids, decoder_input_ids=decoder_input_ids, device=device)\n",
    "        # entropy\n",
    "        squeeze_prob = prob.squeeze()\n",
    "        show_top_k(squeeze_prob, tokenizer= tokenizer)\n",
    "        ent_of_pred = entropy(squeeze_prob.cpu().numpy())\n",
    "        logger.info(f\"Entropy: {ent_of_pred: 8.2f}\")\n",
    "        last_pred = [torch.argmax(squeeze_prob).tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "document = \"The RoBERTa model was proposed in RoBERTa: A Robustly Optimized BERT Pretraining Approach by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov. It is based on Google’s BERT model released in 2018. It builds on BERT and modifies key hyperparameters, removing the next-sentence pretraining objective and training with much larger mini-batches and learning rates.\"\n",
    "device = 'cpu'\n",
    "doc_input_ids = tokenizer(document, return_tensors='pt')['input_ids'][:,:600]\n",
    "decoder_input_ids = torch.tensor([[0]], dtype=torch.long)\n",
    "# print(doc_input_ids.size())\n",
    "# print(decoder_input_ids.size())\n",
    "\n",
    "output, prob, next_token_logits, loss = run_full_model_slim(model=model, input_ids=doc_input_ids, decoder_input_ids=decoder_input_ids, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_pos(inp_str):\n",
    "    inp_str = inp_str.strip()\n",
    "    doc = nlp(inp_str)\n",
    "    for token in doc:\n",
    "        return token.pos_\n",
    "        # print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search\n",
    "from scipy.stats import entropy\n",
    "from typing import List\n",
    "\n",
    "class SingleStep():\n",
    "    def __init__(self, parent, t, prob, entropy, token=\"\", token_id=0):\n",
    "        self.parent = parent\n",
    "        self.t = t\n",
    "        self.prob=prob\n",
    "        self.entropy=entropy\n",
    "        self.token = token\n",
    "        self.token_id = token_id\n",
    "        self.is_compressive = False     # a new and blue job vs a job\n",
    "        self.is_finished = False\n",
    "    def classify_fact(self):\n",
    "        pass\n",
    "\n",
    "    def understand_word_type(self):\n",
    "        pass\n",
    "\n",
    "    def get_future_prediction(self):\n",
    "        pass\n",
    "\n",
    "    def get_token_ids(self):\n",
    "        if self.parent == None:\n",
    "            return [self.token_id]\n",
    "        else:\n",
    "            return self.parent.get_token_ids() + [self.token_id]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Generate Path: {tokenizer.decode(self.get_token_ids(),skip_special_tokens=True)} \"\n",
    "\n",
    "    def run(self):\n",
    "        # expand the node, return a list of new nodes (sorted by the score)\n",
    "        prefix_token_ids = self.get_token_ids()\n",
    "        decoder_input_ids = torch.tensor([prefix_token_ids], dtype=torch.long)\n",
    "        next_step_gen = step_generation(model, doc_input_ids,decoder_input_ids,device)\n",
    "        is_collide = entity_collide(next_step_gen, prefix_token_ids)\n",
    "        if is_collide:\n",
    "            return []\n",
    "\n",
    "        # predict even further TODO\n",
    "\n",
    "        # populate\n",
    "        rt_list = []\n",
    "        logger.info(f\"Populating...\")\n",
    "        logger.info(f\"Prefix: {tokenizer.decode(prefix_token_ids,skip_special_tokens=True)}\")\n",
    "        for element in next_step_gen:\n",
    "            new_ele = SingleStep(parent=self,t=self.t+1, prob=element['prob'], entropy=element['ent'], token=element['token'], token_id=element['token_id'])\n",
    "            if element['token'].strip() == '.' or element['token'] == '</s>':\n",
    "                new_ele.is_finished = True\n",
    "            rt_list.append(new_ele)\n",
    "            logger.info(f\"\\t--- Expansion: {element['token']}\")\n",
    "        logger.info(f\"------\")\n",
    "        return rt_list[::-1]\n",
    "\n",
    "ban_list = ['PROPN','NUM']\n",
    "def entity_collide(generated_group:List, prefix_token_ids):\n",
    "    # a bunch of collided entities / hallucination\n",
    "    # could be a more complicated fact classifier\n",
    "    cand_entropy = generated_group[0]['ent']\n",
    "    if cand_entropy < 3:\n",
    "        return False\n",
    "    \n",
    "    poss = [x['pos'] for x in generated_group]\n",
    "    num_ban_pos = sum([ 1 for x in poss if x in ban_list])\n",
    "    ban_rate = num_ban_pos / len(generated_group)\n",
    "    if ban_rate > 0.4:\n",
    "        logger.info(f\"Entity collides! Entropy: {cand_entropy} Prefix:{tokenizer.decode(prefix_token_ids,skip_special_tokens=True)}\")\n",
    "        logger.info(f\"{[x['token'] for x in generated_group]}\")\n",
    "        return True\n",
    "    logger.info(f\"HIgh entropy (wo Collision) Entropy: {cand_entropy} Prefix: {tokenizer.decode(prefix_token_ids,skip_special_tokens=True)}\")\n",
    "    logger.info(f\"{[x['token'] for x in generated_group]}\")\n",
    "    return False\n",
    "\n",
    "def step_generation(model, doc_input_ids, decoder_input_ids, device, acc_prob_threshold = 0.8, max_beam_size = 5):\n",
    "    # print(doc_input_ids.size())\n",
    "    # print(decoder_input_ids.size())\n",
    "    output, prob, next_token_logits, loss = run_full_model_slim(model=model, input_ids=doc_input_ids, decoder_input_ids=decoder_input_ids, device=device)\n",
    "    ent = entropy(prob.squeeze().cpu().numpy())\n",
    "    acc_prob = 0\n",
    "    top_prob, top_index = torch.topk(input=prob, k=max_beam_size)\n",
    "    top_prob = top_prob.squeeze().tolist()\n",
    "    top_index = top_index.squeeze().tolist()\n",
    "    return_pairs = []\n",
    "    for idx in range(max_beam_size):\n",
    "        value = top_prob[idx]\n",
    "        token_id = top_index[idx]\n",
    "        token = tokenizer.decode(token_id)\n",
    "        return_pairs.append(\n",
    "            {'prob':value,\n",
    "            'token':token,\n",
    "            'token_id':token_id,\n",
    "            'ent':ent,\n",
    "            'pos':get_pos(token)\n",
    "            })\n",
    "        # if the accumulated prob > threshold, stop iteration\n",
    "        acc_prob += value\n",
    "        if acc_prob > acc_prob_threshold:\n",
    "            break\n",
    "    # return something with token tokenid and prob\n",
    "    return return_pairs\n",
    "\n",
    "def future_generation(prefix, future_step=3):\n",
    "\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "\n",
    "document = \"The RoBERTa model was proposed in RoBERTa: A Robustly Optimized BERT Pretraining Approach by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov. It is based on Google’s BERT model released in 2018. It builds on BERT and modifies key hyperparameters, removing the next-sentence pretraining objective and training with much larger mini-batches and learning rates.\"\n",
    "device = 'cpu'\n",
    "doc_input_ids = tokenizer(document, return_tensors='pt')['input_ids'][:,:600]\n",
    "\n",
    "stack = []\n",
    "# print(tokenizer.bos_token)\n",
    "# print(tokenizer.bos_token_id)\n",
    "init_state = SingleStep(None, 0, prob=1.0, entropy=1e-3, token=tokenizer.bos_token, token_id=tokenizer.bos_token_id)\n",
    "stack.append(init_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "finished_output = []\n",
    "T = 200\n",
    "t = 0\n",
    "max_steps = 20\n",
    "while stack:\n",
    "    work_node = stack.pop()\n",
    "    return_output = work_node.run()\n",
    "    for rt in return_output:\n",
    "        if rt.is_finished or rt.t >= max_steps:\n",
    "            finished_output.append(rt)\n",
    "        else:\n",
    "            # stack.insert(0,rt)\n",
    "            stack.append(rt)\n",
    "    t+=1\n",
    "    if t >T:\n",
    "        break\n",
    "\n",
    "logger.info(\"Finished output\")\n",
    "[logger.info(x) for x in finished_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_xsum = load_dataset('xsum',split='validation')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 50\n",
    "device = 'cuda:0'\n",
    "model = model.to(device=device)\n",
    "for data in dataset_xsum:\n",
    "    document = data['document']\n",
    "    summary = data['summary']\n",
    "    logger.info('*'*50)\n",
    "    logger.info(f\"Summary: {summary}\")\n",
    "    # init\n",
    "    doc_input_ids = tokenizer(document, return_tensors='pt')['input_ids'][:,:600]\n",
    "\n",
    "    stack = []\n",
    "    init_state = SingleStep(None, 0, prob=1.0, entropy=1e-3, token=tokenizer.bos_token, token_id=tokenizer.bos_token_id)\n",
    "    stack.append(init_state)\n",
    "\n",
    "    finished_output = []\n",
    "    T = 200\n",
    "    t = 0\n",
    "    max_steps = 20\n",
    "    while stack:\n",
    "        work_node = stack.pop()\n",
    "        return_output = work_node.run()\n",
    "        for rt in return_output:\n",
    "            if rt.is_finished or rt.t >= max_steps:\n",
    "                finished_output.append(rt)\n",
    "            else:\n",
    "                # stack.insert(0,rt)\n",
    "                stack.append(rt)\n",
    "        t+=1\n",
    "        if t > T:\n",
    "            break\n",
    "\n",
    "    logger.info(\"Finished output\")\n",
    "    [logger.info(x) for x in finished_output]\n",
    "    cnt -= 1\n",
    "    if cnt <=0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f518a11ac55f4d3cb26f153e70c63bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=26.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23bc6774a2d64332aa6a558af820543b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898822.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0db1ce529b84f9e9faccb040549d96a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3831b6c480174926862d0a71bcd365f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1355863.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
    "import logging\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-xsum\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-xsum\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>\n",
      "<pad>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.eos_token)\n",
    "print(tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"One of the world's smallest newspapers has won one of the most prestigious awards in the world of journalism - the Pulitzer Prize for Feature Photography, it has been announced on the eve of the awards ceremony in Washington, D.C. and the winners will be announced in New York on Monday.\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ARTICLE_TO_SUMMARIZE = \"It is one of the most prestigious honors bestowed upon journalists and people in the arts. And today, the Pulitzer prize for journalism went to The Post and Courier newspaper of Charleston, South Carolina  - which has a tiny staff of just 80 and a daily circulation of 85,000. The paper's powerful photo series entitled 'Till Death Do Us Part,' on domestic violence scooped the top award for its exploration into why South Carolina is among the deadliest states for women. Winner: This iconic photo by New York Times photographer Daniel Berehulak, was part of a winning series, and shows James Dorbor, 8, suspected of being infected with Ebola, being carried by medical staff to an Ebola treatment center in Monrovia, Liberia. Death: This photograph released by the Pulitzer Board on 20 April 2015 shows one of the winning photographs by Daniel Berehulak who won the 2015 Pulitzer Prize for Feature Photography. </s> One small newspaper\"\n",
    "\n",
    "# ARTICLE_TO_SUMMARIZE =\"\"\n",
    "\n",
    "inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')\n",
    "\n",
    "# Generate Summary\n",
    "summary_ids = model.generate(inputs['input_ids'], num_beams=10, min_length=60, max_length=60, early_stopping=True)\n",
    "print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f9cb647ed887>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ARTICLE_TO_SUMMARIZE =\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mARTICLE_TO_SUMMARIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Generate Summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "ARTICLE_TO_SUMMARIZE = \"It is one of the most prestigious honors bestowed upon journalists and people in the arts. And today, the Pulitzer prize for journalism went to The Post and Courier newspaper of Charleston, South Carolina  - which has a tiny staff of just 80 and a daily circulation of 85,000. The paper's powerful photo series entitled 'Till Death Do Us Part,' on domestic violence scooped the top award for its exploration into why South Carolina is among the deadliest states for women. Winner: This iconic photo by New York Times photographer Daniel Berehulak, was part of a winning series, and shows James Dorbor, 8, suspected of being infected with Ebola, being carried by medical staff to an Ebola treatment center in Monrovia, Liberia. Death: This photograph released by the Pulitzer Board on 20 April 2015 shows one of the winning photographs by Daniel Berehulak who won the 2015 Pulitzer Prize for Feature Photography\"\n",
    "\n",
    "# ARTICLE_TO_SUMMARIZE =\"\"\n",
    "\n",
    "inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')\n",
    "\n",
    "# Generate Summary\n",
    "summary_ids = model.generate(inputs['input_ids'], num_beams=10, min_length=5, max_length=5, early_stopping=True,num_return_sequences=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b2768bf9bd6425acc869120a7136bda7c2db4d32a4c0d82d14afba9d9f0b9d5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "3b2768bf9bd6425acc869120a7136bda7c2db4d32a4c0d82d14afba9d9f0b9d5"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
