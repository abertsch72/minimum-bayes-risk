{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd03b2768bf9bd6425acc869120a7136bda7c2db4d32a4c0d82d14afba9d9f0b9d5",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "annual annual ADJ JJ ROOT xxxx True False\nEuropean european ADJ JJ ROOT Xxxxx True False\nAC AC PROPN NNP ROOT XX True False\n2018 2018 NUM CD ROOT dddd False False\nConference conference NOUN NN ROOT Xxxxx True False\nInternational international ADJ JJ ROOT Xxxxx True False\nNeural neural ADJ JJ ROOT Xxxxx True False\nAmerican American PROPN NNP ROOT Xxxxx True False\nconference conference NOUN NN ROOT xxxx True False\nNational national ADJ JJ ROOT Xxxxx True False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "text = [' annual', ' European', ' AC', ' 2018', ' Conference', ' International', ' Neural', ' American', ' conference', ' National']\n",
    "for t in text:\n",
    "    doc = nlp(t.strip())\n",
    "    for token in doc:\n",
    "        print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "                token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "`num_return_sequences` has to be smaller or equal to `num_beams`.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-985886b1ed55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mARTICLE_TO_SUMMARIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Generate Summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msummary_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msummary_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnum_return_sequences\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`num_return_sequences` has to be smaller or equal to `num_beams`.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m             beam_scorer = BeamSearchScorer(\n",
      "\u001b[0;31mValueError\u001b[0m: `num_return_sequences` has to be smaller or equal to `num_beams`."
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-xsum')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-xsum')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------------------------------\n",
      "Beam Size:10\tDiverse Beam Group #:1\n",
      "A pre-training model for natural language translation has been proposed by a team of researchers at the University of Oxford.\n",
      "A pre-training model for natural language translation has been proposed by a group of researchers at the University of Oxford.\n",
      "A pre-training model for natural language translation has been proposed by researchers at the University of California, Berkeley.\n",
      "A pre-training model for natural language translation has been proposed by a team of researchers at the University of Edinburgh.\n",
      "A pre-training model for natural language translation has been proposed by a team of researchers at the University of Cambridge.\n",
      "A pre-training model for natural language translation has been proposed by a group of researchers at the University of Edinburgh.\n",
      "A pre-training model for natural language translation has been proposed by researchers at the University of Oxford.\n",
      "A pre-training model for machine translation has been proposed by a team of researchers at the University of Oxford.\n",
      "A pre-training model for natural language translation has been proposed by researchers at the University of Edinburgh.\n",
      "A pre-training model for natural language translation has been proposed by a team of researchers.\n",
      "------------------------------\n",
      "Beam Size:100\tDiverse Beam Group #:1\n",
      "A pre-training model for natural language translation has been proposed by a team of researchers at the University of California, Berkeley.\n",
      "A pre-training model for natural language translation has been proposed by a team of researchers at the Massachusetts Institute of Technology.\n",
      "A pre-training model for natural language translation has been proposed by a group of researchers at the University of California, Berkeley.\n",
      "A pre-training model for natural language translation has been proposed by a team of researchers at the University of Oxford.\n",
      "A pre-training model for natural language translation has been proposed by a group of researchers at the Massachusetts Institute of Technology.\n",
      "A pre-training model for natural language translation has been proposed by researchers at the Massachusetts Institute of Technology (MIT).\n",
      "A pre-training model for natural language translation has been proposed by a group of researchers at the University of Oxford.\n",
      "A pre-training model for natural language translation has been proposed by researchers at the Massachusetts Institute of Technology.\n",
      "A pre-training model for natural language translation has been proposed by researchers at the University of California, Berkeley.\n",
      "A pre-trained model for natural language translation has been proposed by a team of researchers at the University of California, Berkeley.\n",
      "------------------------------\n",
      "Beam Size:10\tDiverse Beam Group #:2\n",
      "A pre-training model for natural language translation has been proposed by a team of researchers at the University of California, Berkeley.\n",
      "A pre-training model for natural language translation has been proposed by a group of researchers at the University of California, Berkeley.\n",
      "A pre-training model for natural language translation has been proposed by a team of researchers at the University of Oxford.\n",
      "A pre-training model for natural language translation has been proposed by a team of researchers at the University of Oxford.\n",
      "A pre-training model for natural language translation has been proposed by a group of researchers at the University of Oxford.\n",
      "A pre-training model for natural language translation has been proposed by a group of researchers at the University of Oxford.\n",
      "A pre-training model for natural language translation has been proposed by researchers at the University of California, Berkeley.\n",
      "A pre-training model for natural language translation has been proposed by researchers at the University of California, Berkeley.\n",
      "A pre-training model for natural language translation has been proposed by researchers at the University of Oxford.\n",
      "A pre-training model for natural language translation has been proposed by researchers at the University of Oxford.\n",
      "------------------------------\n",
      "Beam Size:100\tDiverse Beam Group #:2\n",
      "A pre-training model for natural language translation has been proposed by a team of researchers at the Massachusetts Institute of Technology.\n",
      "A pre-training model for natural language translation has been proposed by a team of researchers at the University of Oxford.\n",
      "A pre-training model for natural language translation has been proposed by a team of researchers at the University of Oxford.\n",
      "A pre-training model for natural language translation has been proposed by a group of researchers at the Massachusetts Institute of Technology.\n",
      "A pre-training model for natural language translation has been proposed by researchers at the Massachusetts Institute of Technology (MIT).\n",
      "A pre-training model for natural language translation has been proposed by researchers at the Massachusetts Institute of Technology (MIT).\n",
      "A pre-training model for natural language translation has been proposed by a group of researchers at the University of Oxford.\n",
      "A pre-training model for natural language translation has been proposed by a group of researchers at the University of Oxford.\n",
      "A pre-training model for natural language translation has been proposed by researchers at the Massachusetts Institute of Technology.\n",
      "A pre-training model for natural language translation has been proposed by researchers at the Massachusetts Institute of Technology.\n",
      "------------------------------\n",
      "Beam Size:10\tDiverse Beam Group #:5\n",
      "A new training model for natural language translation has been proposed by a team of researchers at the University of Oxford.\n",
      "A new training model for natural language translation has been proposed by a team of researchers at the University of Oxford.\n",
      "A new training model for natural language translation has been proposed by a team of researchers at the University of Oxford.\n",
      "A new training model for natural language translation has been proposed by a team of researchers at the University of Oxford.\n",
      "A new training model for natural language translation has been proposed by a team of researchers at the University of Oxford.\n",
      "A new training model for natural language translation has been proposed by a group of researchers at the University of Oxford.\n",
      "A new training model for natural language translation has been proposed by a group of researchers at the University of Oxford.\n",
      "A new training model for natural language translation has been proposed by a group of researchers at the University of Oxford.\n",
      "A new training model for natural language translation has been proposed by a group of researchers at the University of Oxford.\n",
      "A new training model for natural language translation has been proposed by a group of researchers at the University of Oxford.\n",
      "------------------------------\n",
      "Beam Size:100\tDiverse Beam Group #:5\n",
      "A pre-training model for natural language translation has been proposed by a team of researchers at the University of Oxford.\n",
      "A pre-training model for natural language translation has been proposed by a team of researchers at the University of Oxford.\n",
      "A pre-training model for natural language translation has been proposed by a team of researchers at the University of Oxford.\n",
      "A pre-training model for natural language translation has been proposed by a team of researchers at the University of Oxford.\n",
      "A pre-training model for natural language translation has been proposed by a group of researchers at the University of Oxford.\n",
      "A pre-training model for natural language translation has been proposed by a group of researchers at the University of Oxford.\n",
      "A pre-training model for natural language translation has been proposed by a group of researchers at the University of Oxford.\n",
      "A pre-training model for natural language translation has been proposed by a group of researchers at the University of Oxford.\n",
      "A pre-training model for natural language translation has been proposed by researchers at the University of California, Berkeley.\n",
      "A pre-training model for natural language translation has been proposed by researchers at the University of California, Berkeley.\n"
     ]
    }
   ],
   "source": [
    "ARTICLE_TO_SUMMARIZE = \"The Bart model was proposed in BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer on 29 Oct, 2019. According to the abstract, Bart uses a standard seq2seq/machine translation architecture with a bidirectional encoder (like BERT) and a left-to-right decoder (like GPT). The pretraining task involves randomly shuffling the order of the original sentences and a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\"\n",
    "inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')\n",
    "# Generate Summary\n",
    "BG=[1,2,5]\n",
    "beam_size = [1, 10, 100]\n",
    "for group_num in BG:\n",
    "    for bs in beam_size:\n",
    "        if bs <= group_num:\n",
    "            continue\n",
    "        print(\"-\"*30)\n",
    "        print(f\"Beam Size:{bs}\\tDiverse Beam Group #:{group_num}\")\n",
    "        summary_ids = model.generate(inputs['input_ids'], num_beams=bs, max_length=100, early_stopping=True,num_return_sequences=10,num_beam_groups=group_num)\n",
    "        print(\"\\n\".join([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}