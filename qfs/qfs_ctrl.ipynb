{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd03b2768bf9bd6425acc869120a7136bda7c2db4d32a4c0d82d14afba9d9f0b9d5",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, PreTrainedTokenizerFast\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"hyunwoongko/ctrlsum-cnndm\")\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"hyunwoongko/ctrlsum-cnndm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'4.3.3'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "inp_doc = r'Mexico City Mayor Claudia Sheinbaum said one of the concrete beams in the overpass in the borough of Tlahuac collapsed as the subway train passed over it. \"The cause of the collapse of the beam must be established through an expert investigation,\" Sheinbaum said. The overpass is on one of Mexico Citys newest subway lines. Line 12 was completed in 2012, according to BBC.com. Mexico Citys El Universal newspaper reported that in 2017, subway riders and residents near the overpass feared the elevated section could collapse because of an earthquake that September. A 7.1 magnitude earthquake hit about 70 miles southeast of Mexico City on Sept. 19. Dozens of buildings collapsed. The death toll eventually reached 369 when the last victim was pulled from the rubble on Oct. 4, 2017.'\n",
    "\n",
    "inp_doc = r\"Relief efforts in Nepal are intensifying after more than 2,300 people were killed in the worst earthquake there in more than 80 years. Rescue missions and aid material have started arriving in the country. Seventeen people have been killed on Mt Everest by avalanches - the mountain's worst-ever disaster. Meanwhile a powerful aftershock was felt on Sunday in Nepal, India and Bangladesh, and more avalanches were reported near Everest. The 6.7-magnitude tremor, centred 60km (40 miles) east of Nepal's capital Kathmandu, sent people running in panic for open ground in the city. At the scene: By Surendra Phuyal, BBC News, Kathmandu As military helicopters hovered over Kathmandu and ferried relief supplies to victims of the earthquake in western Nepal, search and rescue efforts were going on in at least five locations in Kathmandu. Near the prime minister's official residence, two bulldozers scoured the rubble of a flattened four-storey house. A bystander said it was the local tax office. The bodies of the four staff inside had been recovered minutes before. As the search and road-clearing efforts got under way at noon, a major aftershock sent people into a panic again. At a nearby open space, people were camped out fearing further tremors. They were short of food and water - clearly hoping and praying that the aftershocks would stop and things would be normal soon.\"\n",
    "\n",
    "inp_doc = r\"Relief efforts in Nepal are intensifying after more than 2,300 people were killed in the worst earthquake there in more than 80 years. Rescue missions and aid material have started arriving in the country.\"\n",
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[    2,   901,    87,   132,     6,  2965,    82,    58,   848,    11,\n             5,  2373,  8969,    89,    11,    55,    87,  1812,   107,     4,\n          8801, 10779,     8,  2887,  1468,    33,   554,  7789,    11,     5,\n           247,     7,   244,     5,  7149,     9,     5,  4463,     4,    20,\n           744,  5831,    16,   421,     7,  1430,    25,    55,  3738,    32,\n           303,     4,    20, 27139,  4575,   242,   168,    34,  2998,    10,\n           194,     9,  1923,    11,     5,   812, 15359, 14785,   257,     8,\n             5,  1079,     9,     5,   247,    16,    15,  5439,    13,   617,\n         33704,   994,     4,    20,   168,    34,    67,   553,    13,   244,\n            31,     5,   758,   435,     7,   244,    19,  3500,  1170,     5]])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nprompt = [\\'cause\\',\\'rescue\\',\\'toll\\', \\'officer\\', \\'location\\',\\'people\\', \\'injured people\\', \\'location and time\\',\\'death toll in total\\',\\'aftershock effect\\']\\nfor p in prompt:\\n    print(f\"\\nPrompt: {p} =>\")\\n    data = tokenizer(f\"{p} => {inp_doc}\", return_tensors=\"pt\")\\n    input_ids, attention_mask = data[\"input_ids\"], data[\"attention_mask\"]\\n    print(tokenizer.batch_decode(model.generate(input_ids, attention_mask=attention_mask, num_beams=5, min_length=50))[0])\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "\n",
    "data = tokenizer(inp_doc, return_tensors=\"pt\")\n",
    "input_ids, attention_mask = data[\"input_ids\"], data[\"attention_mask\"]\n",
    "decoded = model.generate(input_ids, attention_mask=attention_mask, num_beams=5,min_length=100,max_length=100)\n",
    "print(decoded)\n",
    "\n",
    "\"\"\"\n",
    "prompt = ['cause','rescue','toll', 'officer', 'location','people', 'injured people', 'location and time','death toll in total','aftershock effect']\n",
    "for p in prompt:\n",
    "    print(f\"\\nPrompt: {p} =>\")\n",
    "    data = tokenizer(f\"{p} => {inp_doc}\", return_tensors=\"pt\")\n",
    "    input_ids, attention_mask = data[\"input_ids\"], data[\"attention_mask\"]\n",
    "    print(tokenizer.batch_decode(model.generate(input_ids, attention_mask=attention_mask, num_beams=5, min_length=50))[0])\n",
    "\"\"\"\n",
    "# Q:What is my name? A: =>\n",
    "# for p in prompt:\n",
    "#     print(f\"QA: {p} =>\")\n",
    "#     data = tokenizer(f\"Q:What is the {p}? A: => {inp_doc}\", return_tensors=\"pt\")\n",
    "#     input_ids, attention_mask = data[\"input_ids\"], data[\"attention_mask\"]\n",
    "#     print(tokenizer.batch_decode(model.generate(input_ids, attention_mask=attention_mask, num_beams=5, min_length=50))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}