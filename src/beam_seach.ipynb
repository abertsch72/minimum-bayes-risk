{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "3b2768bf9bd6425acc869120a7136bda7c2db4d32a4c0d82d14afba9d9f0b9d5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "\n",
    "\n",
    "class Beam(object):\n",
    "    \"\"\"\n",
    "    Beam data structure. Maintains a list of scored elements like a Counter, but only keeps the top n\n",
    "    elements after every insertion operation. Insertion is O(n) (list is maintained in\n",
    "    sorted order), access is O(1). Still fast enough for practical purposes for small beams.\n",
    "    \"\"\"\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.elts = []\n",
    "        self.scores = []\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Beam(\" + repr(list(self.get_elts_and_scores())) + \")\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.elts)\n",
    "\n",
    "    def add(self, elt, score):\n",
    "        \"\"\"\n",
    "        Adds the element to the beam with the given score if the beam has room or if the score\n",
    "        is better than the score of the worst element currently on the beam\n",
    "\n",
    "        :param elt: element to add\n",
    "        :param score: score corresponding to the element\n",
    "        \"\"\"\n",
    "        if len(self.elts) == self.size and score < self.scores[-1]:\n",
    "            # Do nothing because this element is the worst\n",
    "            return\n",
    "        # If the list contains the item with a lower score, remove it\n",
    "        i = 0\n",
    "        while i < len(self.elts):\n",
    "            if self.elts[i] == elt and score > self.scores[i]:\n",
    "                del self.elts[i]\n",
    "                del self.scores[i]\n",
    "            i += 1\n",
    "        # If the list is empty, just insert the item\n",
    "        if len(self.elts) == 0:\n",
    "            self.elts.insert(0, elt)\n",
    "            self.scores.insert(0, score)\n",
    "        # Find the insertion point with binary search\n",
    "        else:\n",
    "            lb = 0\n",
    "            ub = len(self.scores) - 1\n",
    "            # We're searching for the index of the first element with score less than score\n",
    "            while lb < ub:\n",
    "                m = (lb + ub) // 2\n",
    "                # Check > because the list is sorted in descending order\n",
    "                if self.scores[m] > score:\n",
    "                    # Put the lower bound ahead of m because all elements before this are greater\n",
    "                    lb = m + 1\n",
    "                else:\n",
    "                    # m could still be the insertion point\n",
    "                    ub = m\n",
    "            # lb and ub should be equal and indicate the index of the first element with score less than score.\n",
    "            # Might be necessary to insert at the end of the list.\n",
    "            if self.scores[lb] > score:\n",
    "                self.elts.insert(lb + 1, elt)\n",
    "                self.scores.insert(lb + 1, score)\n",
    "            else:\n",
    "                self.elts.insert(lb, elt)\n",
    "                self.scores.insert(lb, score)\n",
    "            # Drop and item from the beam if necessary\n",
    "            if len(self.scores) > self.size:\n",
    "                self.elts.pop()\n",
    "                self.scores.pop()\n",
    "\n",
    "    def get_elts(self):\n",
    "        return self.elts\n",
    "\n",
    "    def get_elts_and_scores(self):\n",
    "        return zip(self.elts, self.scores)\n",
    "\n",
    "    def head(self):\n",
    "        return self.elts[0]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-xsum')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-xsum')\n",
    "device = 'cuda:0'\n",
    "\n",
    "# util\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import statistics\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "import multiprocessing\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BartForConditionalGeneration, BartModel, BartTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "logger = logging.getLogger('sum')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# create file handler which logs even debug messages\n",
    "fh = logging.FileHandler(f\"{now.strftime('%m')}{now.strftime('%d')}.html\")\n",
    "fh.setLevel(logging.DEBUG)\n",
    "# create console handler with a higher log level\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "# create formatter and add it to the handlers\n",
    "formatter = logging.Formatter('<br>%(levelname)s - %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "fh.setFormatter(formatter)\n",
    "# add the handlers to logger\n",
    "logger.addHandler(ch)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "from util import *\n",
    "\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "def write_pkl_to_disk(path: str, fname_prefix: str, data_obj):\n",
    "    full_fname = os.path.join(path, f\"{fname_prefix}.pkl\")\n",
    "    with open(full_fname, 'wb') as fd:\n",
    "        pickle.dump(data_obj, fd)\n",
    "    logging.debug(f\"Done writing to {full_fname}\")\n",
    "\n",
    "\n",
    "def init_bart_sum_model(mname='sshleifer/distilbart-cnn-6-6', device='cuda:0'):\n",
    "    model = BartForConditionalGeneration.from_pretrained(mname).to(device)\n",
    "    tokenizer = BartTokenizer.from_pretrained(mname)\n",
    "    return model, tokenizer\n",
    "\n",
    "def bart_decoder_forward_embed(input_ids, embed_tokens, embed_scale):\n",
    "    input_shape = input_ids.size()\n",
    "    input_ids = input_ids.view(-1, input_shape[-1])\n",
    "    inputs_embeds = embed_tokens(input_ids) * embed_scale\n",
    "    return inputs_embeds\n",
    "\n",
    "\n",
    "def summarize_attributions(attributions):\n",
    "    attributions = attributions.mean(dim=-1)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    return attributions\n",
    "\n",
    "def forward_enc_dec_step(model, encoder_outputs, decoder_inputs_embeds):\n",
    "    # expanded_batch_idxs = (\n",
    "    #         torch.arange(batch_size)\n",
    "    #             .view(-1, 1)\n",
    "    #             .repeat(1, 1)\n",
    "    #             .view(-1)\n",
    "    #             .to(device)\n",
    "    #     )\n",
    "    # encoder_outputs[\"last_hidden_state\"] = encoder_outputs.last_hidden_state.index_select(\n",
    "    #         0, expanded_batch_idxs\n",
    "    #     )\n",
    "    model_inputs = {\"input_ids\": None,\n",
    "                    \"past_key_values\": None,\n",
    "                    \"encoder_outputs\": encoder_outputs,\n",
    "                    \"decoder_inputs_embeds\": decoder_inputs_embeds,\n",
    "                    }\n",
    "    outputs = model(**model_inputs, use_cache=False,\n",
    "                    return_dict=True, output_attentions=True)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def init_bart_family(name_lm, name_sum, device, no_lm=False, no_ood=False):\n",
    "    if not no_lm:\n",
    "        lm_model, tok = init_bart_lm_model(name_lm, device)\n",
    "    else:\n",
    "        lm_model = None\n",
    "    sum_model, tok = init_bart_sum_model(name_sum, device)\n",
    "    if not no_ood:\n",
    "        if name_sum == \"facebook/bart-large-cnn\": \n",
    "            sum_out_of_domain, _ = init_bart_sum_model(\n",
    "            \"facebook/bart-large-xsum\", device)\n",
    "        else:\n",
    "            sum_out_of_domain, _ = init_bart_sum_model(\n",
    "            \"facebook/bart-large-cnn\", device) \n",
    "    else:\n",
    "        sum_out_of_domain = None\n",
    "    return lm_model, sum_model, sum_out_of_domain, tok\n",
    "\n",
    "from captum.attr._utils.visualization import format_word_importances\n",
    "\n",
    "\n",
    "def simple_viz_attribution(tokenizer, input_ids, attribution_scores):\n",
    "    token_in_list = input_ids.tolist()\n",
    "    if isinstance(token_in_list[0], list):\n",
    "        token_in_list = token_in_list[0]\n",
    "    words = [tokenizer.decode(x) for x in token_in_list]\n",
    "    attribution_scores_list = attribution_scores.tolist()\n",
    "    # for w, ascore in zip(words, attribution_scores_list):\n",
    "    #     logging.info('{:10} {:02.2f}'.format(w, ascore))\n",
    "\n",
    "    output = format_word_importances(words, attribution_scores_list)\n",
    "    return output\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_full_model_slim(model, input_ids, attention_mask=None, decoder_input_ids=None, targets=None, device='cuda:0', output_dec_hid=False, output_attentions=False, T=1, special_attn=False):\n",
    "    decoder_input_ids = decoder_input_ids.to(device)\n",
    "    input_ids = input_ids.to(device)\n",
    "    if attention_mask is not None:\n",
    "        attention_mask = attention_mask.to(device)\n",
    "    assert decoder_input_ids.size()[0] == input_ids.size()[0]\n",
    "\n",
    "    model_inputs = {\"input_ids\": input_ids,\n",
    "                    \"attention_mask\": attention_mask,\n",
    "                    \"decoder_input_ids\": decoder_input_ids,\n",
    "                    }\n",
    "\n",
    "    outputs = model(**model_inputs,\n",
    "                    output_hidden_states=output_dec_hid, output_attentions=output_attentions,\n",
    "                    use_cache=False, return_dict=True)\n",
    "\n",
    "    # batch, dec seq, vocab size\n",
    "    next_token_logits = outputs.logits[:, -1, :]\n",
    "    if targets is not None:\n",
    "        targets = targets.to(device)\n",
    "        loss = torch.nn.functional.cross_entropy(\n",
    "            input=next_token_logits, target=targets, reduction='none')\n",
    "    else:\n",
    "        loss = 0\n",
    "    if special_attn:\n",
    "        cross_attn = outputs['cross_attentions']\n",
    "        attn = cross_attn[-1][:, :, -1, :]\n",
    "        # batch, nhead, enc_len\n",
    "        mean_attn = torch.mean(attn, dim=1)\n",
    "        # block special positions in input\n",
    "        mask = (input_ids >= 5).float()\n",
    "        mean_attn = mean_attn * mask\n",
    "        return mean_attn[0] \n",
    "    if output_attentions:\n",
    "        # use cross attention as the distribution\n",
    "        # last layer.   batch=1, head, dec len, enc len\n",
    "        # by default we use the last layer of attention\n",
    "        output, p = get_cross_attention(\n",
    "            outputs['cross_attentions'], input_ids, device=device)\n",
    "        return output, p\n",
    "\n",
    "    \n",
    "    prob = torch.nn.functional.softmax(next_token_logits/T, dim=-1)\n",
    "    # prob = next_token_logits.softmax(dim=-1)\n",
    "    next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "    # next_token = next_token.unsqueeze(-1)\n",
    "    next_token = next_token.tolist()    # confrim nested list?\n",
    "    # print(f\"Gold: {tokenizer.decode(targets[0].item())}\")\n",
    "    output = [tokenizer.decode(tk) for tk in next_token]\n",
    "    # logging.info(f\"Next token: {output}\")\n",
    "    # outputs['output'] = output\n",
    "    return output, prob, next_token_logits, loss\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def load_xsum(split='validation'):\n",
    "    from datasets import load_dataset\n",
    "    dataset_xsum = load_dataset('xsum',split=split)\n",
    "    return dataset_xsum\n",
    "source_data = load_xsum()\n",
    "model = model.to(device=device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default\n",
      "WARNING:datasets.builder:Reusing dataset xsum (/home/jcxu/.cache/huggingface/datasets/xsum/default/1.2.0/4957825a982999fbf80bca0b342793b01b2611e021ef589fb7c6250b3577b499)\n",
      "WARNING:datasets.builder:Using custom data configuration default\n",
      "WARNING:datasets.builder:Reusing dataset xsum (/home/jcxu/.cache/huggingface/datasets/xsum/default/1.2.0/4957825a982999fbf80bca0b342793b01b2611e021ef589fb7c6250b3577b499)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# based on current prediction, predict future k steps\n",
    "from typing import List\n",
    "\n",
    "from transformers.generation_utils import top_k_top_p_filtering\n",
    "\n",
    "def future_simulation(model, device, input_doc_token_ids, prefix_token_ids:List, max_expand_steps=5,min_expand_prob=0.1): # return all of the possible tokens\n",
    "    copy_of_prefix_token_ids = prefix_token_ids.copy()\n",
    "    generated_token_ids = []\n",
    "    for t in range(max_expand_steps):\n",
    "        dec_prefix = torch.tensor([prefix_token_ids],dtype=torch.long).to(device)\n",
    "        output, prob, next_token_logits, loss  = run_full_model_slim(model,input_doc_token_ids, decoder_input_ids=dec_prefix)\n",
    "        # print(next_token_logits)\n",
    "        # print(next_token_logits.size())\n",
    "        # break\n",
    "        # squeezed_prob = prob\n",
    "        # print(squeezed_prob[:10])\n",
    "        \n",
    "        filtered_logits = top_k_top_p_filtering(next_token_logits, top_p=0.9, filter_value=0)\n",
    "        probabilities = torch.nn.functional.softmax(filtered_logits, dim=-1)\n",
    "        # print(torch.sum(logits))\n",
    "        next_tokens = torch.multinomial(probabilities, num_samples=1)\n",
    "        print(next_tokens.size())\n",
    "        print(next_tokens)\n",
    "        break\n",
    "        best_id = torch.argmax(next_token_logits.squeeze())\n",
    "        prefix_token_ids += [best_id]\n",
    "        generated_token_ids.append(best_id)\n",
    "    logger.info(f\"Simulation Prefix: {tokenizer.decode(copy_of_prefix_token_ids,skip_special_tokens=True)} | {tokenizer.decode(generated_token_ids,skip_special_tokens=True)}\")\n",
    "    return generated_token_ids\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "for data in source_data:\n",
    "\n",
    "    document = data['document']\n",
    "    summary = data['summary']\n",
    "    break\n",
    "\n",
    "doc_input_ids = tokenizer(document, return_tensors='pt')['input_ids'][:,:600]\n",
    "doc_input_ids = doc_input_ids.to(device)\n",
    "summary_ids = model.generate(doc_input_ids, num_beams=1, max_length=100, early_stopping=True)\n",
    "summary_ids =summary_ids.squeeze().tolist()\n",
    "prefix = summary_ids[:5]\n",
    "output = future_simulation(model, device, doc_input_ids, prefix )\n",
    "print(output)\n",
    "def run_baseline_bs():\n",
    "    pass"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<br>INFO - Simulation Prefix: Apple has been accused | \n",
      "INFO:sum:Simulation Prefix: Apple has been accused | \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 1])\n",
      "tensor([[9]], device='cuda:0')\n",
      "[]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "for data in source_data:\n",
    "\n",
    "    document = data['document']\n",
    "    summary = data['summary']\n",
    "    break\n",
    "\n",
    "document = \"The Bart model was proposed in BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer on 29 Oct, 2019. According to the abstract, Bart uses a standard seq2seq/machine translation architecture with a bidirectional encoder (like BERT) and a left-to-right decoder (like GPT). The pretraining task involves randomly shuffling the order of the original sentences and a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\"\n",
    "\n",
    "\n",
    "inputs = tokenizer([document], max_length=1024, return_tensors='pt')\n",
    "\n",
    "# Generate Summary\n",
    "for t in range(3, 10):\n",
    "    summary_ids = model.generate(inputs['input_ids'].to(device), num_beams=5, min_length=1, max_length=t, early_stopping=True, num_return_sequences=5)\n",
    "    outs = []\n",
    "    for g in summary_ids:\n",
    "        out = tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "        outs.append(out)\n",
    "    outs.sort()\n",
    "    print('-------',t)\n",
    "    print('\\t\\t'.join(outs))\n",
    "\n",
    "# print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------- 3\n",
      "A\t\tAn\t\tResearchers\t\tScientists\t\tThe\n",
      "------- 4\n",
      "A model\t\tA new\t\tA novel\t\tA pre\t\tA training\n",
      "------- 5\n",
      "A model for\t\tA model that\t\tA new model\t\tA new training\t\tA pre-\n",
      "------- 6\n",
      "A model that pret\t\tA new model for\t\tA new training model\t\tA pre-trained\t\tA pre-training\n",
      "------- 7\n",
      "A new model for training\t\tA new training model for\t\tA pre-trained model\t\tA pre-training algorithm\t\tA pre-training model\n",
      "------- 8\n",
      "A new training model for natural\t\tA pre-trained model for\t\tA pre-training algorithm for\t\tA pre-training model for\t\tA pre-training model that\n",
      "------- 9\n",
      "A new training model for natural language\t\tA pre-trained model for natural\t\tA pre-training algorithm for natural\t\tA pre-training model for artificial\t\tA pre-training model for natural\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}